# -*- coding: utf-8 -*-
"""RAGRESPONSE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C3ja-Nqtexc8lOE4kjDTVY2Nwo6wKbx1
"""

!pip install python-dotenv -q

!pip install langchain_google_genai -q

!pip install langchain -q

!pip install langchain_community -q

import bs4
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter

import getpass
import os

if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Google AI API key: ")
GOOGLE_API_KEY = "AIzaSyDBR53kk5SVMmVacCf2oysPoTDWUAbpBo4"

import getpass
import os
from dotenv import load_dotenv

load_dotenv()

if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("AIzaSyA2NDYc5CSWAKZWItk4xLf5dT5znp18aNY")

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    # other params...
)

messages = [
    (
        "system",
        "You are a helpful assistant that translates English to French. Translate the user sentence.",
    ),
    ("human", "I love programming."),
]
ai_msg = llm.invoke(messages)
ai_msg
print(ai_msg.content)

from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant that translates {input_language} to {output_language}.",
        ),
        ("human", "{input}"),
    ]
)

chain = prompt | llm
print(chain.invoke(
    {
        "input_language": "English",
        "output_language": "German",
        "input": "I love programming.",
    }
))

import os

os.environ["HUGGINGFACE_API_TOKEN"] = "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"

!pip install sentence_transformers -q



from langchain.embeddings import SentenceTransformerEmbeddings
embeddings =SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)

docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = InMemoryVectorStore(embeddings)
vectorstore.add_documents(splits)
retriever = vectorstore.as_retriever()

from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

!pip install langgraph -q

from typing import Sequence

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict


# We define a dict representing the state of the application.
# This state has the same input and output keys as `rag_chain`.
class State(TypedDict):
    input: str
    chat_history: Annotated[Sequence[BaseMessage], add_messages]
    context: str
    answer: str


# We then define a simple node that runs the `rag_chain`.
# The `return` values of the node update the graph state, so here we just
# update the chat history with the input message and response.
def call_model(state: State):
    response = rag_chain.invoke(state)
    return {
        "chat_history": [
            HumanMessage(state["input"]),
            AIMessage(response["answer"]),
        ],
        "context": response["context"],
        "answer": response["answer"],
    }


# Our graph consists only of one node:
workflow = StateGraph(state_schema=State)
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# Finally, we compile the graph with a checkpointer object.
# This persists the state, in this case in memory.
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

config = {"configurable": {"thread_id": "abc123"}}

result = app.invoke(
    {"input": "What is Task Decomposition?"},
    config=config,
)
print(result["answer"])

result = app.invoke(
    {"input": "What is one way of doing it?"},
    config=config,
)
print(result["answer"])

chat_history = app.get_state(config).values["chat_history"]
for message in chat_history:
    message.pretty_print()

!pip install FastAPI -q
!pip install uvicorn -q
!pip install nest_asyncio -q
!pip install pyngrok -q

from fastapi import FastAPI
from langgraph.graph import START, StateGraph
from langchain_core.messages import AIMessage, HumanMessage
from typing import Sequence
from typing_extensions import Annotated, TypedDict
import nest_asyncio
from pyngrok import ngrok
import uvicorn
import json
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://127.0.0.1:5500"],  # Allow your local origin
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods
    allow_headers=["*"],  # Allow all headers
)

# Define a dict representing the state of the application.
class State(TypedDict):
    input: str
    chat_history: Annotated[Sequence[BaseMessage], add_messages]
    context: str
    answer: str

# Function to run the RAG chain and return a response
def call_model(state: State):
    response = rag_chain.invoke(state)
    return {
        "chat_history": [
            HumanMessage(state["input"]),
            AIMessage(response["answer"]),
        ],
        "context": response["context"],
        "answer": response["answer"],
    }

# Define a graph consisting of one node to run the RAG chain
workflow = StateGraph(state_schema=State)
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# In-memory chat history and state saver
memory = MemorySaver()
app_workflow = workflow.compile(checkpointer=memory)

# FastAPI endpoint for asking questions
@app.get("/ask")
async def ask_question(input: str):
    config = {"configurable": {"thread_id": "abc123"}}

    # Invoke the app with the user's input
    result = app_workflow.invoke(
        {"input": input},
        config=config,
    )

    return {
        "answer": result["answer"],
        "context": result["context"]
    }

# FastAPI endpoint to retrieve chat history
@app.get("/history")
async def get_history():
    config = {"configurable": {"thread_id": "abc123"}}
    chat_history = app_workflow.get_state(config).values["chat_history"]

    history = []
    for message in chat_history:
        history.append(message.content)

    return {"chat_history": history}

# Run the FastAPI server (use uvicorn in the terminal)
# Example: uvicorn myfile:app --reload


auth_token = "2n1qBqzGTh3820ja9hLHi6c5xnI_5Tw4YgWezveSw2BXRGh64"
ngrok.set_auth_token(auth_token)
ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)

# Ensure nest_asyncio is applied for Colab environments
nest_asyncio.apply()

# Run the FastAPI server
uvicorn.run(app, port=8000)

from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, Request
from pydantic import BaseModel
from typing import List, Optional
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, START

!pip install nest_asyncio -q
!pip install pyngrok -q

!pip install chromadb -q

from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import AIMessage, HumanMessage
from langchain.chains import ConversationalRetrievalChain

from langchain.vectorstores import Chroma
import logging

app = FastAPI()

# Setup logging
logging.basicConfig(level=logging.INFO)

# Global variable to maintain chat history
chat_history = []

# Initialize the vector store (Chroma) to manage document embeddings
chroma_db = Chroma()

# Initialize the LLM (Gemini via Langchain)
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0.7,  # Adjusted for varied responses
    max_tokens=300,   # Increased to allow longer responses
    timeout=None,
    max_retries=2
)

# Define RAG Chain with Conversational Retrieval
rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=chroma_db.as_retriever()
)

# Define system prompt for reformulating questions with context
contextualize_q_system_prompt = (
    "Given the chat history and the latest user question, "
    "formulate a standalone question that can be understood without the chat history. "
    "Do NOT answer the question, just reformulate it."
)

# Function to call the model, ensuring chat history and context are passed properly
def call_model(state: dict):
    # Log the context before making the model call to debug
    logging.info(f"Context before model call: {state['context']}")

    # Make the call to the RAG model, passing input, chat history, and context
    response = rag_chain.invoke({
        "input": state["input"],           # The current user input/question
        "chat_history": state["chat_history"],  # Full chat history
        "context": state["context"]        # Retrieved context
    })

    # Return the updated state with chat history and response
    return {
        "chat_history": state["chat_history"] + [
            HumanMessage(state["input"]),
            AIMessage(response["answer"])
        ],  # Append user input and AI response to the chat history
        "context": response["context"],  # Updated context from the AI response
        "answer": response["answer"]    # AI's response based on input and context
    }

# FastAPI endpoint to handle chat
@app.get("/chat")
async def chat(input: str):
    global chat_history

    # Initialize the state with current input and chat history
    state = {
        "input": input,
        "chat_history": chat_history,
        "context": "",
        "answer": ""
    }

    # Call the model with the current state
    response = call_model(state)

    # Update the global chat history with the latest conversation
    chat_history = response["chat_history"]

    # Return the user's input, AI's response, updated context, and full chat history
    return {
        "user_input": input,
        "response": response["answer"],
        "context": response["context"],  # Return the context
        "chat_history": [
            {"user": msg.content} if isinstance(msg, HumanMessage) else {"bot": msg.content}
            for msg in chat_history
        ]  # Format the chat history for easier display
    }

# Optional: Endpoint to clear the chat history (if needed)
@app.get("/reset_chat")
async def reset_chat():
    global chat_history
    chat_history = []  # Reset chat history
    return {"message": "Chat history cleared."}


# Set up ngrok for public access
auth_token = "2n1qBqzGTh3820ja9hLHi6c5xnI_5Tw4YgWezveSw2BXRGh64"
ngrok.set_auth_token(auth_token)
ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)

# Ensure nest_asyncio is applied for Colab environments
nest_asyncio.apply()

# Run the FastAPI server
uvicorn.run(app, port=8000)

import nest_asyncio
from pyngrok import ngrok
from fastapi import FastAPI
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

# Create FastAPI instance
app = FastAPI()

# Set up CORS middleware to allow local frontend requests
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://127.0.0.1:5500"],  # Allow your local origin
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods
    allow_headers=["*"],  # Allow all headers
)

# Define chat history and request models
class QuestionRequest(BaseModel):
    question: str
    chat_history: Optional[List[dict]] = []

# Chat history to store conversation state
chat_history = []

# Function to simulate model response (replace with actual model invocation)
def call_model(input_text: str, chat_history: List[dict]):
    # Simulate a response from the model (replace this with actual RAG model call)
    return {
        "answer": f"Response to '{input_text}'",
        "context": "Relevant context here"
    }

# Define the API endpoint to handle question asking
@app.post("/ask")
async def ask_question(request: QuestionRequest):
    global chat_history

    # Extract the input and chat history
    user_input = request.question
    chat_history = request.chat_history or chat_history

    # Simulate calling the model
    response = call_model(user_input, chat_history)

    # Append the current interaction to the chat history
    chat_history.append({"user": user_input, "bot": response["answer"]})

    return {"answer": response["answer"], "context": response["context"], "chat_history": chat_history}



# Set up ngrok for public access
auth_token = "2n1qBqzGTh3820ja9hLHi6c5xnI_5Tw4YgWezveSw2BXRGh64"
ngrok.set_auth_token(auth_token)
ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)

# Ensure nest_asyncio is applied for Colab environments
nest_asyncio.apply()

# Run the FastAPI server
uvicorn.run(app, port=8000)

# Define the initial chat history as an empty list
chat_history = []

# Create a GET request to handle chat input via query parameters
@app.get("/chat")
async def chat(input: str):
    global chat_history

    # Create the state for the RAG chain model
    state = {
        "input": input,
        "chat_history": chat_history,
        "context": "",
        "answer": ""
    }

    # Call the RAG chain model (using the call_model function from your code)
    response = call_model(state)

    # Update chat history with new input and response
    chat_history.extend(response["chat_history"])

    return {
        "user_input": input,
        "response": response["answer"]
    }

auth_token = "2n1qBqzGTh3820ja9hLHi6c5xnI_5Tw4YgWezveSw2BXRGh64"
ngrok.set_auth_token(auth_token)
ngrok_tunnel = ngrok.connect(8000)
print('Public URL:', ngrok_tunnel.public_url)

nest_asyncio.apply()
uvicorn.run(app, port=8000)







app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)

class QuestionRequest(BaseModel):
    question: str
    chat_history: Optional[List[dict]] = []  # To support chat history if provided

# Create the FastAPI route to handle question asking
@app.post("/ask")
async def ask_question(request: QuestionRequest):
    # Prepare the input question
    user_input = request.question
    config = {"configurable": {"thread_id": "abc123"}}

    # If there's chat history, load it into the app state (simulating this)
    if request.chat_history:
        memory.load_state(request.chat_history)  # Load previous chat history

    # Invoke the app with the new question
    response = workflow.invoke({"input": user_input}, config=config)

    # Return the model's answer
    return {"answer": response["answer"], "context": response["context"]}

